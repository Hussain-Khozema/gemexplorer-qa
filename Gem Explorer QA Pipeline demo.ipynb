{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c3f750",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29aeb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2021 22:42:04 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
      "06/22/2021 22:42:04 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "06/22/2021 22:42:04 - INFO - faiss.loader -   Loading faiss.\n",
      "06/22/2021 22:42:06 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "06/22/2021 22:42:08 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
      "06/22/2021 22:42:08 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "06/22/2021 22:42:08 - INFO - faiss.loader -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from haystack.preprocessor.cleaning import clean_wiki_text\n",
    "from haystack.preprocessor.preprocessor import PreProcessor\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http, eval_data_from_json\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.document_store import InMemoryDocumentStore\n",
    "from haystack.retriever.sparse import TfidfRetriever\n",
    "from haystack.utils import print_answers\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.pipeline import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91bf19",
   "metadata": {},
   "source": [
    "#### Check Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673007a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e138a84",
   "metadata": {},
   "source": [
    "#### Connective to ElasticSearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a175b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2021 22:42:09 - INFO - elasticsearch -   HEAD http://localhost:9200/ [status:200 request:0.028s]\n",
      "06/22/2021 22:42:09 - INFO - elasticsearch -   HEAD http://localhost:9200/document [status:200 request:0.011s]\n",
      "06/22/2021 22:42:09 - INFO - elasticsearch -   GET http://localhost:9200/document [status:200 request:0.004s]\n",
      "06/22/2021 22:42:09 - INFO - elasticsearch -   PUT http://localhost:9200/document/_mapping [status:200 request:0.020s]\n",
      "06/22/2021 22:42:09 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.003s]\n"
     ]
    }
   ],
   "source": [
    "# Connect to Elasticsearch\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "        host=\"localhost\",\n",
    "        port=9200,\n",
    "        text_field = 'text',\n",
    "        name_field = 'name',\n",
    "        username=\"\",\n",
    "        password=\"\",\n",
    "        index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94bf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd876a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2021 22:42:09 - INFO - farm.utils -   Using device: CUDA \n",
      "06/22/2021 22:42:09 - INFO - farm.utils -   Number of GPUs: 1\n",
      "06/22/2021 22:42:09 - INFO - farm.utils -   Distributed Training: False\n",
      "06/22/2021 22:42:09 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
      "06/22/2021 22:42:11 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": true, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "06/22/2021 22:42:15 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "06/22/2021 22:42:15 - INFO - farm.utils -   Using device: CUDA \n",
      "06/22/2021 22:42:15 - INFO - farm.utils -   Number of GPUs: 1\n",
      "06/22/2021 22:42:15 - INFO - farm.utils -   Distributed Training: False\n",
      "06/22/2021 22:42:15 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
      "06/22/2021 22:42:15 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...\n",
      "06/22/2021 22:42:15 - INFO - farm.infer -    0    0    0    0    0    0    0 \n",
      "06/22/2021 22:42:15 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "06/22/2021 22:42:15 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n",
      "06/22/2021 22:42:15 - INFO - farm.infer -               \n"
     ]
    }
   ],
   "source": [
    "# Loading trained model\n",
    "\n",
    "reader = FARMReader('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0567628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f66e2e",
   "metadata": {},
   "source": [
    "#### Prediction Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d0e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_retriever=5\n",
    "top_k_reader=3\n",
    "def get_answer(question, top_answer= False):\n",
    "    prediction = pipe.run(query=question, top_k_retriever=top_k_retriever, top_k_reader=top_k_reader)\n",
    "    if len(prediction['answers']) == 0:\n",
    "        return \"No answers found\"\n",
    "    print(\"Top Answer = \", prediction['answers'][0]['answer'])\n",
    "    print(\"-\"* 100)\n",
    "    answers = prediction['answers']\n",
    "    if not top_answer:\n",
    "        for answer in answers:\n",
    "            ans = answer[\"answer\"]\n",
    "            context = answer[\"context\"]\n",
    "            document_name = answer[\"meta\"][\"name\"]\n",
    "            score = answer[\"score\"]\n",
    "            print(\"Answer: \\n\", ans)\n",
    "            print(\"\\nContext: \\n\", context)\n",
    "            print(\"\\nDocument_name: \\n\", document_name)\n",
    "            print(\"\\nScore: \\n\", score)\n",
    "            print('-' * 100)\n",
    "    return prediction['answers'][0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43973c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2021 22:42:15 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.109s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.23s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.51 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.51 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.52 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.74 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Answer =  Students at a more senior level with similar merits shall have a higher chance of allocation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer: \n",
      " Students at a more senior level with similar merits shall have a higher chance of allocation.\n",
      "\n",
      "Context: \n",
      " s for last competitive spot)\n",
      "Students at a more senior level with similar merits shall have a higher chance of allocation. Students with prior oversea\n",
      "\n",
      "Document_name: \n",
      " allocation_criteria.txt\n",
      "\n",
      "Score: \n",
      " 11.602807998657227\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer: \n",
      " Please do not make any payment fee for any administrative matter including visa, air tickets and accommodation until you are advised to do so.\n",
      "\n",
      "Context: \n",
      " Please do not make any payment fee for any administrative matter including visa, air tickets and accommodation until you are advised to do so.\n",
      "\n",
      "Document_name: \n",
      " dates_and_deadlines.txt\n",
      "\n",
      "Score: \n",
      " 5.387704849243164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer: \n",
      " given a one-time priority for placement\n",
      "\n",
      "Context: \n",
      "  and TSP Scholars who are eligible for GEM Explorer are given a one-time priority for placement in any overseas programme managed by OGEM. This means \n",
      "\n",
      "Document_name: \n",
      " scholars_guarantee.txt\n",
      "\n",
      "Score: \n",
      " 4.882081985473633\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Students at a more senior level with similar merits shall have a higher chance of allocation.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"Do senior students get any priority?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eba15d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2021 23:49:14 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.005s]\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "\n",
    "def printData(firstName, lastName):\n",
    "    print(firstName)\n",
    "    print(lastName)\n",
    "    root.destroy()\n",
    "\n",
    "def print_answer():\n",
    "    question = entry1.get()\n",
    "    answer = get_answer(question, False)\n",
    "    answer_label.config(text=\"Answer: {}\".format(answer))\n",
    "\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "#question label\n",
    "label1 = Label(root,text = 'Enter your question')\n",
    "label1.pack()\n",
    "label1.config(justify = CENTER)\n",
    "\n",
    "entry1 = Entry(root, width = 100)\n",
    "entry1.pack()\n",
    "\n",
    "button1 = Button(root, text = 'submit', command = print_answer)\n",
    "button1.pack() \n",
    "button1.config(command = print_answer)\n",
    "\n",
    "answer_label = Label(root,text = '')\n",
    "answer_label.pack()\n",
    "answer_label.config(justify = CENTER)\n",
    "    \n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d27dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
